{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'facenet_pytorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_719621/4019356933.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlipreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLipreading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarp_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcut_patch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mSTD_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/IntuitDetect/preprocessing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mface_detector\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/IntuitDetect/preprocessing/face_detector.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfacenet_pytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmtcnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMTCNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'facenet_pytorch'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import deque\n",
    "from contextlib import contextmanager\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import face_alignment\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "\n",
    "from lipreading.model import Lipreading\n",
    "from preprocessing.transform import warp_img, cut_patch\n",
    "\n",
    "STD_SIZE = (256, 256)\n",
    "STABLE_PNTS_IDS = [33, 36, 39, 42, 45]\n",
    "START_IDX = 48\n",
    "STOP_IDX = 68\n",
    "CROP_WIDTH = CROP_HEIGHT = 96\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def VideoCapture(*args, **kwargs):\n",
    "    cap = cv2.VideoCapture(*args, **kwargs)\n",
    "    try:\n",
    "        yield cap\n",
    "    finally:\n",
    "        cap.release()\n",
    "\n",
    "\n",
    "def load_model(config_path: Path):\n",
    "    with config_path.open() as fp:\n",
    "        config = json.load(fp)\n",
    "    tcn_options = {\n",
    "        'num_layers': config['tcn_num_layers'],\n",
    "        'kernel_size': config['tcn_kernel_size'],\n",
    "        'dropout': config['tcn_dropout'],\n",
    "        'dwpw': config['tcn_dwpw'],\n",
    "        'width_mult': config['tcn_width_mult'],\n",
    "    }\n",
    "    return Lipreading(\n",
    "        num_classes=500,\n",
    "        tcn_options=tcn_options,\n",
    "        backbone_type=config['backbone_type'],\n",
    "        relu_type=config['relu_type'],\n",
    "        width_mult=config['width_mult'],\n",
    "        extract_feats=False,\n",
    "    )\n",
    "\n",
    "\n",
    "def visualize_probs(vocab, probs, col_width=4, col_height=300):\n",
    "    num_classes = len(probs)\n",
    "    out = np.zeros((col_height, num_classes * col_width + (num_classes - 1), 3), dtype=np.uint8)\n",
    "    for i, p in enumerate(probs):\n",
    "        x = (col_width + 1) * i\n",
    "        cv2.rectangle(out, (x, 0), (x + col_width - 1, round(p * col_height)), (255, 255, 255), 1)\n",
    "    top = np.argmax(probs)\n",
    "    cv2.addText(out, f'Prediction: {vocab[top]}', (10, out.shape[0] - 30), 'Arial', color=(255, 255, 255))\n",
    "    cv2.addText(out, f'Confidence: {probs[top]:.3f}', (10, out.shape[0] - 10), 'Arial', color=(255, 255, 255))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "queue_length = 30\n",
    "\n",
    "fa = face_alignment.FaceAlignment(face_alignment.LandmarksType.TWO_D, device=device)\n",
    "model = load_model('configs/lrw_resnet18_mstcn.json')\n",
    "model.load_state_dict(torch.load(Path('models/lrw_resnet18_mstcn_video.pth.tar'), map_location=device)['model_state_dict'])\n",
    "model = model.to(device)\n",
    "\n",
    "mean_face_landmarks = np.load(Path('preprocessing/20words_mean_face.npy'))\n",
    "\n",
    "with Path('labels/500WordsSortedList.txt').open() as fp:\n",
    "    vocab = fp.readlines()\n",
    "assert len(vocab) == 500\n",
    "\n",
    "queue = deque(maxlen=queue_length)\n",
    "\n",
    "cap = cv2.VideoCapture('/home/miranjo/IntuitDetect/data/fake/a_numbers_fake.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    ret, image_np = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    image_np = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    all_landmarks = fa.get_landmarks(image_np)\n",
    "    if all_landmarks:\n",
    "        landmarks = all_landmarks[0]\n",
    "        \n",
    "        # BEGIN PROCESSING\n",
    "        trans_frame, trans = warp_img(\n",
    "            landmarks[STABLE_PNTS_IDS, :], mean_face_landmarks[STABLE_PNTS_IDS, :], image_np, STD_SIZE)\n",
    "        trans_landmarks = trans(landmarks)\n",
    "        patch = cut_patch(\n",
    "            trans_frame, trans_landmarks[START_IDX:STOP_IDX], CROP_HEIGHT // 2, CROP_WIDTH // 2)\n",
    "        cv2.imshow('patch', cv2.cvtColor(patch, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "        patch_torch = to_tensor(cv2.cvtColor(patch, cv2.COLOR_RGB2GRAY)).to(device)\n",
    "        queue.append(patch_torch)\n",
    "\n",
    "        if len(queue) >= queue_length:\n",
    "            with torch.no_grad():\n",
    "                model_input = torch.stack(list(queue), dim=1).unsqueeze(0)\n",
    "                logits = model(model_input, lengths=[queue_length])\n",
    "                probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "                probs = probs[0].detach().cpu().numpy()\n",
    "            vis = visualize_probs(vocab, probs)\n",
    "            cv2.imshow('probs', vis)\n",
    "            # END PROCESSING\n",
    "\n",
    "            for x, y in landmarks:\n",
    "                cv2.circle(image_np, (int(x), int(y)), 2, (0, 0, 255))\n",
    "\n",
    "            cv2.imshow('camera', cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "            key = cv2.waitKey(1)\n",
    "            if key in {27, ord('q')}:  # 27 is Esc\n",
    "                break\n",
    "            elif key == ord(' '):\n",
    "                cv2.waitKey(0)\n",
    "\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
